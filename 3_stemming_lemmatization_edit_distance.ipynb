{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process of stripping off affixes to find basic morphological structre \n",
    "or reduceing a word to its stem/root/base form\n",
    "\n",
    "different variants of a term can be conflated to a single representative form \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Stemming with NLTK'''\n",
    "import nltk\n",
    "from nltk.stem.porter import * \n",
    "stemmer = PortStemmer()\n",
    "words = ['grasses','flies','mules','denied','matched','agreed','motoring','making','traditional','rational','colonial','reference','itemization','duration']\n",
    "stems = [stemmer.stem(w) for w in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors in Stemming \n",
    "\n",
    "* Commission / FP\n",
    "    * inclide affix when it should not have been\n",
    "* Omission / FN\n",
    "    * exclude affix when it should not have\n",
    "\n",
    "|Comission Erros| Omission Erros|\n",
    "|---|---|\n",
    "|doing &rarr; doe(do)|organization =>organ(organize)|\n",
    "|generalization &rarr; generic(general) | matrices &rarr; matric(matrice)|\n",
    "|numerical &rarr; numerous(numeric)|nosiy &rarr; noisi(noise)|\n",
    "|policy &rarr; police(policy) | urgency &rarr; urgenc(urgent)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Comission & Ommision Error with Porter Stemmer '''\n",
    "\n",
    "sent = \"Stemming is easier than morphological analysis, says the sushi loving computer scientist\"\n",
    "\n",
    "stem = [''.join(stemmer.stem(stems)) for stems in sent.split()]\n",
    "print(stem)\n",
    "\n",
    "plurals = ['caresses','flies','dies','mules','denied','died','agreed','owned','humbled','sized','meeting','stating','siezing','itemization','sensational','traditional','reference','colonizer','plotted']\n",
    "\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors in Stemming II\n",
    "\n",
    "* Understemming ,FN\n",
    "    * Two seperate words that should be stemmed to the same root , but are not\n",
    "* Overstemming , FP\n",
    "    * two seperate words that are stemmed to the same root but should not have been , FP\n",
    "\n",
    "| Understemming Errors | Overstemming Errors | \n",
    "|---|---|\n",
    "| dividing , divided &rarr; divide </break> division , divisor &rarr; divise (divide) | dividing , devided &rarr; divide divine , divination &rarr; divide(divine)|\n",
    "\n",
    "\n",
    "\n",
    "## Lemmatization \n",
    "\n",
    " * A lemma is the canonical or dictionary form of a set of related words.\n",
    "    * pay - is the lemma for paying , paid and pays\n",
    "\n",
    "* A lemma usually , but not ncessarily resembles the words it is related to\n",
    "    * be - is the lemma of is was and am\n",
    "* Unlike stemming , lemmatisation noy only tries to group related words togethe r, but also group words by their word sense or meaning.\n",
    "* The same word may represent two different meanings.\n",
    "* Lemmatization requires the understaind of context , this is a more complicated and expensive process as comapared to stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Lemmatization with NLTK'''\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize(\"pays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Segmentation\n",
    "\n",
    "* The process of segmenting/tokenizing text into words\n",
    "* Common languages like English using Alphabet easily seperate words by spaces.\n",
    "* Segementin gpurely on white spaces is not enough , has to address erros by treating punctuations as word boundry.\n",
    "* Language with special characters such as Chinese, Japense and Thai cannot easily seperated \n",
    "    * english sentence : Enter the room\n",
    "    * Chinese sentence \n",
    "* Segementation may also invoke tokenizng multiple expressions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
